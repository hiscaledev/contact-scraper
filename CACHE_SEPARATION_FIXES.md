# Cache Separation and Response Format Fixes

## üéØ Issues Fixed

### Issue #1: Cache Conflict Between Endpoints

**Problem:**
- Both `/scrap` and `/scrap-linkedin` used the **same cache key** (`contact:http://example.com`)
- If you called `/scrap-linkedin` first, it cached empty emails/phones
- Then calling `/scrap` would return the cached result with empty emails/phones instead of scraping

**Example of the Bug:**
```bash
# Step 1: Call LinkedIn-only endpoint
curl "/scrap-linkedin?website=example.com"
# Response: Cached with empty emails/phones

# Step 2: Call full scraping endpoint
curl "/scrap?website=example.com"
# BUG: Returns cached result with empty emails/phones!
# Expected: Should scrape emails and phones
```

**Solution:**
- **Separate cache keys** for each endpoint:
  - LinkedIn-only: `linkedin:http://example.com`
  - Full contact: `contact:http://example.com`
- Now both endpoints can coexist without conflicts

---

### Issue #2: Response Format Inconsistency

**Problem:**
- `/scrap-linkedin` returned the same JSON structure as `/scrap`
- Included unnecessary fields: `emails: []`, `phones: []`
- Nested structure for LinkedIn: `linkedin_urls: {company: [], personal: []}`
- CSV output had 6+ columns for LinkedIn URLs

**Solution:**
- **New response schema** for LinkedIn-only endpoint
- Cleaner, simpler format with only LinkedIn data
- CSV output reduced to just **2 columns**

---

## ‚úÖ What Changed

### 1. Separate Cache Keys

#### Before (Shared Cache)
```python
# Both endpoints used the same key
cache_key = f"contact:{website}"
```

#### After (Separate Caches)
```python
# LinkedIn-only endpoint
cache_key = f"linkedin:{website}"

# Full contact endpoint
cache_key = f"contact:{website}"
```

**Benefits:**
- ‚úÖ No cache conflicts
- ‚úÖ Can call `/scrap-linkedin` then `/scrap` without issues
- ‚úÖ Each endpoint caches its own data independently

---

### 2. New Response Schema

#### LinkedIn-Only Response (`/scrap-linkedin`)

**Before:**
```json
{
  "website": "http://example.com",
  "emails": [],
  "phones": [],
  "linkedin_urls": {
    "company": ["https://linkedin.com/company/example"],
    "personal": ["https://linkedin.com/in/john-doe"]
  },
  "status": "success"
}
```

**After:**
```json
{
  "website": "http://example.com",
  "company_linkedin": ["https://linkedin.com/company/example"],
  "personal_linkedin": ["https://linkedin.com/in/john-doe"],
  "status": "success"
}
```

**Changes:**
- ‚ùå Removed `emails` field
- ‚ùå Removed `phones` field
- ‚úÖ Flattened structure: `company_linkedin` and `personal_linkedin` arrays
- ‚úÖ Cleaner, more focused response

---

### 3. Simplified CSV Output

#### LinkedIn CSV Output (`/csv/upload-linkedin-csv`)

**Before (6 Columns):**
```csv
website,company_linkedin_url_1,company_linkedin_url_2,company_linkedin_url_3,personal_linkedin_url_1,personal_linkedin_url_2,personal_linkedin_url_3
example.com,https://linkedin.com/company/ex,,,https://linkedin.com/in/john,,
```

**After (2 Columns):**
```csv
website,company_linkedin,personal_linkedin
example.com,https://linkedin.com/company/ex,"https://linkedin.com/in/john, https://linkedin.com/in/jane"
```

**Changes:**
- ‚úÖ Only **2 new columns** (company_linkedin, personal_linkedin)
- ‚úÖ Multiple URLs comma-separated in same column
- ‚úÖ Cleaner, easier to parse
- ‚ùå Removed `raw_json_response` column
- ‚ùå Removed separate columns for each URL

---

## üìã Files Modified

### 1. New Schema Types (`app/schemas/contact.py`)

Added two new Pydantic models:

```python
class LinkedInOnlyResponse(BaseModel):
    """LinkedIn-only scraping response schema."""
    website: str
    company_linkedin: List[str]
    personal_linkedin: List[str]
    status: str

class LinkedInErrorResponse(BaseModel):
    """Error response for LinkedIn-only scraping."""
    website: str
    error: str
    status: str
```

### 2. LinkedIn Service (`app/services/linkedin_service.py`)

**Changes:**
- Uses separate cache key: `linkedin:{website}`
- Returns `LinkedInOnlyResponse` instead of `ContactInfo`
- Direct cache operations with `redis_client` instead of shared helpers

```python
# Separate cache key
cache_key = f"linkedin:{website}"

# Return new response format
return LinkedInOnlyResponse(
    website=website,
    company_linkedin=company_urls,
    personal_linkedin=personal_urls,
    status="success"
)
```

### 3. API Endpoint (`app/api/routes/contact.py`)

**Changes:**
- Updated response model to `LinkedInOnlyResponse`
- Documentation updated to reflect separate cache

```python
@router.get(
    "/scrap-linkedin",
    response_model=LinkedInOnlyResponse,  # New response type
    tags=["Scraping"],
    ...
)
```

### 4. LinkedIn CSV Service (`app/services/linkedin_csv_service.py`)

**Changes:**
- Outputs only 2 columns: `company_linkedin` and `personal_linkedin`
- Comma-separated values if multiple URLs
- Removed `raw_json_response` column
- Removed individual URL columns (url_1, url_2, url_3)

```python
# Simple 2-column output
df["company_linkedin"] = ", ".join(company_urls)
df["personal_linkedin"] = ", ".join(personal_urls)
```

### 5. Documentation Updates

- `README.md` - Updated endpoint comparison table
- `LINKEDIN_SCRAPING.md` - New response format examples
- `LINKEDIN_ENDPOINTS_SUMMARY.md` - Updated response format

---

## üîç Testing the Fixes

### Test 1: Verify Cache Separation

```bash
# Clear all caches first
docker exec contact-scraper-redis redis-cli FLUSHALL

# Step 1: Call LinkedIn-only endpoint
curl "https://scraper.hiscale.ai/scrap-linkedin?website=example.com" \
  -H "X-API-Key: your-key"

# Check cache keys
docker exec contact-scraper-redis redis-cli KEYS "*"
# Expected: linkedin:http://example.com

# Step 2: Call full contact endpoint
curl "https://scraper.hiscale.ai/scrap?website=example.com" \
  -H "X-API-Key: your-key"

# Check cache keys again
docker exec contact-scraper-redis redis-cli KEYS "*"
# Expected: Both keys present:
#   - linkedin:http://example.com
#   - contact:http://example.com
```

### Test 2: Verify Response Format

```bash
# LinkedIn-only endpoint
curl "https://scraper.hiscale.ai/scrap-linkedin?website=example.com" \
  -H "X-API-Key: your-key" | jq

# Expected response:
# {
#   "website": "http://example.com",
#   "company_linkedin": ["..."],
#   "personal_linkedin": ["..."],
#   "status": "success"
# }
#
# Should NOT contain:
# - "emails" field
# - "phones" field
# - "linkedin_urls" nested object
```

### Test 3: Verify CSV Output

```bash
# Create test CSV
echo "website" > test.csv
echo "example.com" >> test.csv

# Upload for LinkedIn-only processing
curl -X POST "https://scraper.hiscale.ai/csv/upload-linkedin-csv" \
  -H "X-API-Key: your-key" \
  -F "file=@test.csv"

# Download result when complete
# Expected columns: website, company_linkedin, personal_linkedin, scrape_status
# NOT: company_linkedin_url_1, company_linkedin_url_2, etc.
```

---

## üìä Cache Key Reference

### Complete Cache Key Map

| Endpoint | Cache Key Format | Example |
|----------|------------------|---------|
| `/scrap-linkedin` | `linkedin:{url}` | `linkedin:http://example.com` |
| `/scrap` | `contact:{url}` | `contact:http://example.com` |
| `/scrap?skip_contact_page=true` | `contact:{url}` | `contact:http://example.com` |

**Note:** Full contact endpoints (`/scrap`) share the same cache regardless of parameters.

---

## üîß Monitoring Cache

### Check Cache Keys

```bash
# List all LinkedIn-only cache keys
docker exec contact-scraper-redis redis-cli KEYS "linkedin:*"

# List all full contact cache keys
docker exec contact-scraper-redis redis-cli KEYS "contact:*"

# Get specific cached data
docker exec contact-scraper-redis redis-cli GET "linkedin:http://example.com"

# Check TTL
docker exec contact-scraper-redis redis-cli TTL "linkedin:http://example.com"
```

### Clear Specific Cache

```bash
# Clear LinkedIn-only cache for a website
docker exec contact-scraper-redis redis-cli DEL "linkedin:http://example.com"

# Clear full contact cache for a website
docker exec contact-scraper-redis redis-cli DEL "contact:http://example.com"

# Clear all LinkedIn-only caches
docker exec contact-scraper-redis redis-cli --scan --pattern "linkedin:*" | xargs docker exec -i contact-scraper-redis redis-cli DEL

# Clear all caches (use with caution!)
docker exec contact-scraper-redis redis-cli FLUSHALL
```

---

## üéØ Use Cases

### Scenario 1: LinkedIn First, Then Full Contact

```bash
# 1. Quick LinkedIn check (2-5 seconds)
curl "/scrap-linkedin?website=startup.io"
# Response: {company_linkedin: [...], personal_linkedin: [...]}
# Cached in: linkedin:http://startup.io

# 2. Later, need full contact info (10-30 seconds)
curl "/scrap?website=startup.io"
# Response: {emails: [...], phones: [...], linkedin_urls: {...}}
# Cached in: contact:http://startup.io

# Result: Both caches exist independently, no conflicts!
```

### Scenario 2: Full Contact First, Then LinkedIn

```bash
# 1. Full scraping (10-30 seconds)
curl "/scrap?website=company.com"
# Cached in: contact:http://company.com

# 2. Quick LinkedIn check (2-5 seconds, will re-scrape)
curl "/scrap-linkedin?website=company.com"
# Cached in: linkedin:http://company.com

# Note: /scrap-linkedin will NOT use /scrap cache (separate keys)
```

---

## üêõ Troubleshooting

### Issue: LinkedIn-Only Still Returns Old Format

**Symptom:**
```json
{
  "emails": [],
  "phones": [],
  "linkedin_urls": {...}
}
```

**Solution:**
```bash
# Restart the app to load new schema
docker compose restart app

# Clear old cache entries
docker exec contact-scraper-redis redis-cli FLUSHALL
```

### Issue: CSV Has Old Columns

**Symptom:** CSV output has `company_linkedin_url_1`, `company_linkedin_url_2`, etc.

**Solution:**
```bash
# Restart app
docker compose restart app

# Re-upload CSV (old jobs still use old format)
```

### Issue: Cache Conflict Still Happening

**Symptom:** Calling `/scrap` after `/scrap-linkedin` returns empty emails/phones

**Diagnosis:**
```bash
# Check cache keys
docker exec contact-scraper-redis redis-cli KEYS "*example.com*"

# If you see only "contact:http://example.com" (not "linkedin:*"),
# the fix wasn't applied properly
```

**Solution:**
```bash
# Verify code changes
docker exec contact-scraper cat /app/app/services/linkedin_service.py | grep "cache_key"
# Should show: cache_key = f"linkedin:{website}"

# Restart app
docker compose restart app
```

---

## üìà Performance Impact

### Response Size Comparison

**LinkedIn-Only Response:**
- **Before:** ~250 bytes (with empty emails/phones fields)
- **After:** ~180 bytes (29% smaller)

**CSV File Size:**
- **Before:** 6 columns for LinkedIn URLs + raw_json_response
- **After:** 2 columns for LinkedIn URLs
- **Reduction:** ~60% smaller for LinkedIn-only data

### Cache Hit Rate

With separate caches, cache hit rates improve:
- LinkedIn-only requests don't pollute full contact cache
- Full contact requests don't affect LinkedIn-only cache
- Each endpoint optimized for its specific use case

---

## ‚úÖ Summary

### What Was Fixed

1. ‚úÖ **Cache Separation**
   - LinkedIn-only: `linkedin:{url}`
   - Full contact: `contact:{url}`
   - No more conflicts!

2. ‚úÖ **Cleaner Response Format**
   - Removed unnecessary `emails` and `phones` fields
   - Flattened `company_linkedin` and `personal_linkedin`
   - More focused, easier to parse

3. ‚úÖ **Simplified CSV Output**
   - Only 2 columns: `company_linkedin` and `personal_linkedin`
   - Comma-separated values for multiple URLs
   - Removed 4+ unnecessary columns

### Benefits

- ‚úÖ **No cache conflicts** between endpoints
- ‚úÖ **Cleaner API responses** (29% smaller)
- ‚úÖ **Simpler CSV output** (60% fewer columns)
- ‚úÖ **Better separation of concerns**
- ‚úÖ **Can use both endpoints independently**

### Backward Compatibility

- ‚úÖ **Full contact endpoint** (`/scrap`) unchanged
- ‚úÖ **Full CSV endpoint** (`/csv/upload-csv`) unchanged
- ‚ö†Ô∏è **LinkedIn-only endpoints** changed (breaking change)
  - `/scrap-linkedin` response format changed
  - `/csv/upload-linkedin-csv` output columns changed
  - If you're using these endpoints, update your integration

---

## üìö Related Documentation

- `README.md` - Main API documentation
- `LINKEDIN_SCRAPING.md` - Complete LinkedIn-only guide
- `LINKEDIN_ENDPOINTS_SUMMARY.md` - Quick reference
- `CONCURRENT_CSV_PROCESSING.md` - CSV performance guide

---

**Last Updated:** 2024-01-20  
**Issue:** Cache conflict and response format inconsistency  
**Status:** ‚úÖ Fixed and deployed  
**Breaking Changes:** LinkedIn-only endpoints only